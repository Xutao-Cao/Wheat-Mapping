{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from Utils.Dataloader import Satellite_image_dataset, get_data\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from Models.U_net import U_net\n",
    "# from Models.wmm import WMM\n",
    "from Models.WMM import WMM\n",
    "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score\n",
    "import torch.nn as nn\n",
    "from Utils.Helper import MultiTrainHelper, train_mission\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(train_loader, classifier, criterion, optimizer):\n",
    "  classifier.train()\n",
    "  loss_ = 0.0\n",
    "  losses = []\n",
    "  for i, (images, labels) in enumerate(train_loader):\n",
    "      images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "      optimizer.zero_grad()\n",
    "      logits = classifier(images)\n",
    "      loss = criterion(logits, labels.squeeze())\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      losses.append(loss)\n",
    "  return torch.stack(losses).mean().item()\n",
    "\n",
    "def test_net(test_loader, classifier, criterion,):\n",
    "    classifier.eval()\n",
    "    losses = []\n",
    "    pred_list = []\n",
    "    label_list = []\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            logits = classifier(images)\n",
    "            loss = criterion(logits, labels.squeeze())\n",
    "            pred = torch.where(torch.sigmoid(logits)>=0.5, 1, 0).cpu().numpy()\n",
    "            pred = np.reshape(pred, (-1,))\n",
    "            labels = np.reshape(labels.cpu().numpy(), (-1,))\n",
    "            label_list.append(labels)\n",
    "            pred_list.append(pred)\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        all_preds = np.concatenate(pred_list)\n",
    "        all_labels = np.concatenate(label_list)\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds)\n",
    "        coppa = cohen_kappa_score(all_labels, all_preds)\n",
    "        test_loss = np.mean(losses)\n",
    "        print(\"Test result:\\n test_loss: {:.4f}, accuracy: {:.4f}, f1: {:.4f}, coppa: {:.4f}\".format(test_loss, acc, f1, coppa))\n",
    "        \n",
    "    return test_loss, acc, f1, coppa\n",
    "def plot_losses(train, val, test_frequency, num_epochs):\n",
    "    plt.plot(train, label=\"train\")\n",
    "    indices = [i for i in range(num_epochs) if ((i+1)%test_frequency == 0 or i ==0)]\n",
    "    plt.plot(indices, val, label=\"val\")\n",
    "    plt.title(\"Loss Plot\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_acc(train, val, test_frequency, num_epochs):\n",
    "    indices = [i for i in range(num_epochs) if ((i+1)%test_frequency == 0 or i ==0)]\n",
    "    plt.plot(indices, train, label=\"train\")\n",
    "    plt.plot(indices, val, label=\"val\")\n",
    "    plt.title(\"Accuracy Plot\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_f1(train, val, test_frequency, num_epochs):\n",
    "    indices = [i for i in range(num_epochs) if ((i+1)%test_frequency == 0 or i ==0)]\n",
    "    plt.plot(indices, train, label=\"train\")\n",
    "    plt.plot(indices, val, label=\"val\")\n",
    "    plt.title(\"F1 Plot\")\n",
    "    plt.ylabel(\"F1\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "def train(classifier, num_epochs, train_loader, val_loader, criterion, optimizer, scheduler, test_frequency=5):\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    train_f1s =[]\n",
    "    val_f1s = []\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        print(\"Starting epoch number \" + str(epoch))\n",
    "        train_loss = train_net(train_loader, classifier, criterion, optimizer)\n",
    "        train_losses.append(train_loss)\n",
    "        print(\"Loss for Training on Epoch \" +str(epoch) + \" is \"+ str(train_loss))\n",
    "        if(epoch%test_frequency==0 or epoch==1):\n",
    "            print('Evaluating classifier')\n",
    "            loss_train, acc_train, f1_train, _ = test_net(train_loader, classifier, criterion)\n",
    "            train_accs.append(acc_train)\n",
    "            train_f1s.append(f1_train)\n",
    "            loss_test, acc_test, f1_test, _ = test_net(val_loader, classifier, criterion)\n",
    "            val_losses.append(loss_test)\n",
    "            val_accs.append(acc_test)\n",
    "            val_f1s.append(f1_test)\n",
    "        scheduler.step(loss_train)\n",
    "    \n",
    "    return classifier, train_losses, val_losses, train_accs, val_accs, train_f1s, val_f1s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-4\n",
    "num_epochs = 60\n",
    "test_frequency = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mission_list = []\n",
    "idx = 0\n",
    "for type in ['L8', 'S1']:\n",
    "  for site in [ 1, 3,5,7,9]:\n",
    "    mission = train_mission(train_site=[site, ], test_site=[site, ] , idx= idx, type=type, model_name='WMM')\n",
    "    train_mission_list.append(mission)\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-04 12:43:47 Index:0, Model: WMM, train_site: 1, test_site: 1, type: L8\n",
      "Starting epoch number 1\n",
      "Loss for Training on Epoch 1 is 0.43976908922195435\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.5881, accuracy: 0.7598, f1: 0.6375, coppa: 0.4755\n",
      "Test result:\n",
      " test_loss: 0.5803, accuracy: 0.6863, f1: 0.3638, coppa: 0.2358\n",
      "Starting epoch number 2\n",
      "Loss for Training on Epoch 2 is 0.3574461042881012\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.3914, accuracy: 0.8519, f1: 0.8325, coppa: 0.7005\n",
      "Test result:\n",
      " test_loss: 0.4150, accuracy: 0.7965, f1: 0.6993, coppa: 0.5500\n",
      "Starting epoch number 3\n",
      "Loss for Training on Epoch 3 is 0.3464890420436859\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.3325, accuracy: 0.8623, f1: 0.8431, coppa: 0.7210\n",
      "Test result:\n",
      " test_loss: 0.4438, accuracy: 0.7837, f1: 0.6665, coppa: 0.5148\n",
      "Starting epoch number 4\n",
      "Loss for Training on Epoch 4 is 0.33332639932632446\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.3255, accuracy: 0.8645, f1: 0.8369, coppa: 0.7210\n",
      "Test result:\n",
      " test_loss: 0.4648, accuracy: 0.7760, f1: 0.6455, coppa: 0.4933\n",
      "Starting epoch number 5\n",
      "Loss for Training on Epoch 5 is 0.3323802947998047\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.3423, accuracy: 0.8607, f1: 0.8370, coppa: 0.7155\n",
      "Test result:\n",
      " test_loss: 0.6528, accuracy: 0.7202, f1: 0.4925, coppa: 0.3397\n",
      "Starting epoch number 6\n",
      "Loss for Training on Epoch 6 is 0.331725150346756\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.3443, accuracy: 0.8572, f1: 0.8388, coppa: 0.7113\n",
      "Test result:\n",
      " test_loss: 0.4807, accuracy: 0.7703, f1: 0.6353, coppa: 0.4799\n",
      "Starting epoch number 7\n",
      "Loss for Training on Epoch 7 is 0.3214825689792633\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.3281, accuracy: 0.8662, f1: 0.8412, coppa: 0.7256\n",
      "Test result:\n",
      " test_loss: 0.5347, accuracy: 0.6897, f1: 0.3715, coppa: 0.2443\n",
      "Starting epoch number 8\n",
      "Loss for Training on Epoch 8 is 0.31803303956985474\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.3995, accuracy: 0.8300, f1: 0.7679, coppa: 0.6378\n",
      "Test result:\n",
      " test_loss: 1.0731, accuracy: 0.6205, f1: 0.0376, coppa: 0.0218\n",
      "Epoch 00008: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Starting epoch number 9\n",
      "Loss for Training on Epoch 9 is 0.3133138418197632\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2965, accuracy: 0.8779, f1: 0.8585, coppa: 0.7514\n",
      "Test result:\n",
      " test_loss: 0.4546, accuracy: 0.7714, f1: 0.6326, coppa: 0.4803\n",
      "Starting epoch number 10\n",
      "Loss for Training on Epoch 10 is 0.30326613783836365\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2953, accuracy: 0.8813, f1: 0.8591, coppa: 0.7566\n",
      "Test result:\n",
      " test_loss: 0.7259, accuracy: 0.6419, f1: 0.1612, coppa: 0.0944\n",
      "Starting epoch number 11\n",
      "Loss for Training on Epoch 11 is 0.30354878306388855\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2946, accuracy: 0.8804, f1: 0.8609, coppa: 0.7562\n",
      "Test result:\n",
      " test_loss: 0.5158, accuracy: 0.7298, f1: 0.5184, coppa: 0.3656\n",
      "Starting epoch number 12\n",
      "Loss for Training on Epoch 12 is 0.3026266098022461\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2892, accuracy: 0.8832, f1: 0.8635, coppa: 0.7615\n",
      "Test result:\n",
      " test_loss: 0.5636, accuracy: 0.7052, f1: 0.4333, coppa: 0.2925\n",
      "Starting epoch number 13\n",
      "Loss for Training on Epoch 13 is 0.29821279644966125\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2811, accuracy: 0.8832, f1: 0.8635, coppa: 0.7615\n",
      "Test result:\n",
      " test_loss: 0.5713, accuracy: 0.6916, f1: 0.3823, coppa: 0.2512\n",
      "Starting epoch number 14\n",
      "Loss for Training on Epoch 14 is 0.2972261309623718\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2927, accuracy: 0.8800, f1: 0.8628, coppa: 0.7566\n",
      "Test result:\n",
      " test_loss: 0.4727, accuracy: 0.7421, f1: 0.5557, coppa: 0.4008\n",
      "Starting epoch number 15\n",
      "Loss for Training on Epoch 15 is 0.29706382751464844\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2840, accuracy: 0.8844, f1: 0.8623, coppa: 0.7627\n",
      "Test result:\n",
      " test_loss: 0.6957, accuracy: 0.6563, f1: 0.2309, coppa: 0.1406\n",
      "Starting epoch number 16\n",
      "Loss for Training on Epoch 16 is 0.2942083179950714\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2822, accuracy: 0.8867, f1: 0.8656, coppa: 0.7678\n",
      "Test result:\n",
      " test_loss: 0.6403, accuracy: 0.6671, f1: 0.2792, coppa: 0.1746\n",
      "Starting epoch number 17\n",
      "Loss for Training on Epoch 17 is 0.30009904503822327\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2851, accuracy: 0.8858, f1: 0.8628, coppa: 0.7650\n",
      "Test result:\n",
      " test_loss: 0.7011, accuracy: 0.6702, f1: 0.2856, coppa: 0.1823\n",
      "Epoch 00017: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Starting epoch number 18\n",
      "Loss for Training on Epoch 18 is 0.2945604920387268\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2887, accuracy: 0.8829, f1: 0.8662, coppa: 0.7624\n",
      "Test result:\n",
      " test_loss: 0.4858, accuracy: 0.7483, f1: 0.5711, coppa: 0.4171\n",
      "Starting epoch number 19\n",
      "Loss for Training on Epoch 19 is 0.282292902469635\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2747, accuracy: 0.8881, f1: 0.8682, coppa: 0.7710\n",
      "Test result:\n",
      " test_loss: 0.5424, accuracy: 0.7098, f1: 0.4452, coppa: 0.3046\n",
      "Starting epoch number 20\n",
      "Loss for Training on Epoch 20 is 0.2918781638145447\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2825, accuracy: 0.8875, f1: 0.8693, coppa: 0.7708\n",
      "Test result:\n",
      " test_loss: 0.5138, accuracy: 0.7272, f1: 0.5036, coppa: 0.3555\n",
      "Starting epoch number 21\n",
      "Loss for Training on Epoch 21 is 0.29248133301734924\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2733, accuracy: 0.8891, f1: 0.8704, coppa: 0.7736\n",
      "Test result:\n",
      " test_loss: 0.6248, accuracy: 0.6946, f1: 0.3920, coppa: 0.2597\n",
      "Starting epoch number 22\n",
      "Loss for Training on Epoch 22 is 0.2882400453090668\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2745, accuracy: 0.8894, f1: 0.8706, coppa: 0.7742\n",
      "Test result:\n",
      " test_loss: 0.5760, accuracy: 0.7006, f1: 0.4163, coppa: 0.2784\n",
      "Starting epoch number 23\n",
      "Loss for Training on Epoch 23 is 0.2851116359233856\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2704, accuracy: 0.8906, f1: 0.8711, coppa: 0.7761\n",
      "Test result:\n",
      " test_loss: 0.6600, accuracy: 0.6808, f1: 0.3378, coppa: 0.2176\n",
      "Starting epoch number 24\n",
      "Loss for Training on Epoch 24 is 0.2803167700767517\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2717, accuracy: 0.8902, f1: 0.8717, coppa: 0.7759\n",
      "Test result:\n",
      " test_loss: 0.6758, accuracy: 0.6799, f1: 0.3371, coppa: 0.2157\n",
      "Starting epoch number 25\n",
      "Loss for Training on Epoch 25 is 0.2801738679409027\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2650, accuracy: 0.8919, f1: 0.8716, coppa: 0.7783\n",
      "Test result:\n",
      " test_loss: 0.6375, accuracy: 0.6800, f1: 0.3366, coppa: 0.2156\n",
      "Starting epoch number 26\n",
      "Loss for Training on Epoch 26 is 0.2781393527984619\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2667, accuracy: 0.8916, f1: 0.8716, coppa: 0.7778\n",
      "Test result:\n",
      " test_loss: 0.6313, accuracy: 0.6706, f1: 0.2932, coppa: 0.1852\n",
      "Starting epoch number 27\n",
      "Loss for Training on Epoch 27 is 0.27854377031326294\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2613, accuracy: 0.8919, f1: 0.8732, coppa: 0.7790\n",
      "Test result:\n",
      " test_loss: 0.6474, accuracy: 0.6823, f1: 0.3406, coppa: 0.2211\n",
      "Starting epoch number 28\n",
      "Loss for Training on Epoch 28 is 0.2797810435295105\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2665, accuracy: 0.8920, f1: 0.8727, coppa: 0.7790\n",
      "Test result:\n",
      " test_loss: 0.5000, accuracy: 0.7275, f1: 0.5026, coppa: 0.3557\n",
      "Starting epoch number 29\n",
      "Loss for Training on Epoch 29 is 0.27957645058631897\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2622, accuracy: 0.8930, f1: 0.8727, coppa: 0.7805\n",
      "Test result:\n",
      " test_loss: 0.6039, accuracy: 0.7037, f1: 0.4231, coppa: 0.2863\n",
      "Starting epoch number 30\n",
      "Loss for Training on Epoch 30 is 0.27617907524108887\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2671, accuracy: 0.8918, f1: 0.8746, coppa: 0.7797\n",
      "Test result:\n",
      " test_loss: 0.5907, accuracy: 0.6989, f1: 0.4115, coppa: 0.2740\n",
      "Starting epoch number 31\n",
      "Loss for Training on Epoch 31 is 0.2788373529911041\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2622, accuracy: 0.8947, f1: 0.8750, coppa: 0.7840\n",
      "Test result:\n",
      " test_loss: 0.5821, accuracy: 0.7249, f1: 0.4937, coppa: 0.3480\n",
      "Epoch 00031: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Starting epoch number 32\n",
      "Loss for Training on Epoch 32 is 0.2744099199771881\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2603, accuracy: 0.8946, f1: 0.8770, coppa: 0.7849\n",
      "Test result:\n",
      " test_loss: 0.5652, accuracy: 0.7181, f1: 0.4768, coppa: 0.3301\n",
      "Starting epoch number 33\n",
      "Loss for Training on Epoch 33 is 0.26961976289749146\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2659, accuracy: 0.8942, f1: 0.8773, coppa: 0.7845\n",
      "Test result:\n",
      " test_loss: 0.6009, accuracy: 0.7048, f1: 0.4360, coppa: 0.2928\n",
      "Starting epoch number 34\n",
      "Loss for Training on Epoch 34 is 0.27445530891418457\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2563, accuracy: 0.8968, f1: 0.8768, coppa: 0.7880\n",
      "Test result:\n",
      " test_loss: 0.6249, accuracy: 0.6921, f1: 0.3814, coppa: 0.2517\n",
      "Starting epoch number 35\n",
      "Loss for Training on Epoch 35 is 0.2703041136264801\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2589, accuracy: 0.8971, f1: 0.8775, coppa: 0.7888\n",
      "Test result:\n",
      " test_loss: 0.6453, accuracy: 0.6812, f1: 0.3378, coppa: 0.2182\n",
      "Starting epoch number 36\n",
      "Loss for Training on Epoch 36 is 0.2718231678009033\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2565, accuracy: 0.8970, f1: 0.8787, coppa: 0.7892\n",
      "Test result:\n",
      " test_loss: 0.6102, accuracy: 0.6885, f1: 0.3681, coppa: 0.2410\n",
      "Starting epoch number 37\n",
      "Loss for Training on Epoch 37 is 0.26597556471824646\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2557, accuracy: 0.8976, f1: 0.8787, coppa: 0.7901\n",
      "Test result:\n",
      " test_loss: 0.6424, accuracy: 0.6791, f1: 0.3317, coppa: 0.2125\n",
      "Starting epoch number 38\n",
      "Loss for Training on Epoch 38 is 0.2680658996105194\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2580, accuracy: 0.8978, f1: 0.8775, coppa: 0.7898\n",
      "Test result:\n",
      " test_loss: 0.6265, accuracy: 0.6761, f1: 0.3146, coppa: 0.2019\n",
      "Starting epoch number 39\n",
      "Loss for Training on Epoch 39 is 0.2674952447414398\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2561, accuracy: 0.8972, f1: 0.8793, coppa: 0.7898\n",
      "Test result:\n",
      " test_loss: 0.6555, accuracy: 0.6830, f1: 0.3479, coppa: 0.2246\n",
      "Starting epoch number 40\n",
      "Loss for Training on Epoch 40 is 0.27138805389404297\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2561, accuracy: 0.8983, f1: 0.8801, coppa: 0.7918\n",
      "Test result:\n",
      " test_loss: 0.6216, accuracy: 0.6885, f1: 0.3667, coppa: 0.2406\n",
      "Starting epoch number 41\n",
      "Loss for Training on Epoch 41 is 0.2657894194126129\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2542, accuracy: 0.8985, f1: 0.8793, coppa: 0.7918\n",
      "Test result:\n",
      " test_loss: 0.6272, accuracy: 0.6903, f1: 0.3729, coppa: 0.2459\n",
      "Starting epoch number 42\n",
      "Loss for Training on Epoch 42 is 0.2637336254119873\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2500, accuracy: 0.8995, f1: 0.8808, coppa: 0.7939\n",
      "Test result:\n",
      " test_loss: 0.6299, accuracy: 0.6846, f1: 0.3512, coppa: 0.2286\n",
      "Starting epoch number 43\n",
      "Loss for Training on Epoch 43 is 0.2667323052883148\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2510, accuracy: 0.8994, f1: 0.8815, coppa: 0.7941\n",
      "Test result:\n",
      " test_loss: 0.6577, accuracy: 0.6862, f1: 0.3611, coppa: 0.2346\n",
      "Starting epoch number 44\n",
      "Loss for Training on Epoch 44 is 0.2620575428009033\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2519, accuracy: 0.8994, f1: 0.8814, coppa: 0.7941\n",
      "Test result:\n",
      " test_loss: 0.6190, accuracy: 0.6805, f1: 0.3355, coppa: 0.2164\n",
      "Starting epoch number 45\n",
      "Loss for Training on Epoch 45 is 0.2601492404937744\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2503, accuracy: 0.9002, f1: 0.8819, coppa: 0.7955\n",
      "Test result:\n",
      " test_loss: 0.6599, accuracy: 0.6956, f1: 0.3934, coppa: 0.2620\n",
      "Starting epoch number 46\n",
      "Loss for Training on Epoch 46 is 0.26069891452789307\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2474, accuracy: 0.9009, f1: 0.8825, coppa: 0.7969\n",
      "Test result:\n",
      " test_loss: 0.6495, accuracy: 0.6826, f1: 0.3463, coppa: 0.2234\n",
      "Starting epoch number 47\n",
      "Loss for Training on Epoch 47 is 0.2647966742515564\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2497, accuracy: 0.9015, f1: 0.8829, coppa: 0.7979\n",
      "Test result:\n",
      " test_loss: 0.6618, accuracy: 0.6837, f1: 0.3476, coppa: 0.2258\n",
      "Starting epoch number 48\n",
      "Loss for Training on Epoch 48 is 0.26100048422813416\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2505, accuracy: 0.9009, f1: 0.8828, coppa: 0.7970\n",
      "Test result:\n",
      " test_loss: 0.5863, accuracy: 0.7098, f1: 0.4424, coppa: 0.3035\n",
      "Starting epoch number 49\n",
      "Loss for Training on Epoch 49 is 0.2634273171424866\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2479, accuracy: 0.9007, f1: 0.8835, coppa: 0.7970\n",
      "Test result:\n",
      " test_loss: 0.5760, accuracy: 0.7076, f1: 0.4425, coppa: 0.2998\n",
      "Starting epoch number 50\n",
      "Loss for Training on Epoch 50 is 0.258192241191864\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2471, accuracy: 0.9018, f1: 0.8840, coppa: 0.7989\n",
      "Test result:\n",
      " test_loss: 0.7197, accuracy: 0.6786, f1: 0.3272, coppa: 0.2103\n",
      "Starting epoch number 51\n",
      "Loss for Training on Epoch 51 is 0.27156075835227966\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2483, accuracy: 0.9016, f1: 0.8832, coppa: 0.7982\n",
      "Test result:\n",
      " test_loss: 0.5799, accuracy: 0.7151, f1: 0.4618, coppa: 0.3197\n",
      "Starting epoch number 52\n",
      "Loss for Training on Epoch 52 is 0.2557498514652252\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2443, accuracy: 0.9017, f1: 0.8852, coppa: 0.7994\n",
      "Test result:\n",
      " test_loss: 0.6623, accuracy: 0.6826, f1: 0.3471, coppa: 0.2238\n",
      "Starting epoch number 53\n",
      "Loss for Training on Epoch 53 is 0.25726932287216187\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2415, accuracy: 0.9045, f1: 0.8864, coppa: 0.8041\n",
      "Test result:\n",
      " test_loss: 0.7165, accuracy: 0.6720, f1: 0.3000, coppa: 0.1899\n",
      "Starting epoch number 54\n",
      "Loss for Training on Epoch 54 is 0.26593106985092163\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2472, accuracy: 0.9033, f1: 0.8843, coppa: 0.8013\n",
      "Test result:\n",
      " test_loss: 0.6042, accuracy: 0.7040, f1: 0.4218, coppa: 0.2864\n",
      "Starting epoch number 55\n",
      "Loss for Training on Epoch 55 is 0.26613906025886536\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2473, accuracy: 0.9001, f1: 0.8840, coppa: 0.7965\n",
      "Test result:\n",
      " test_loss: 0.5684, accuracy: 0.7180, f1: 0.4721, coppa: 0.3284\n",
      "Starting epoch number 56\n",
      "Loss for Training on Epoch 56 is 0.25568273663520813\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2436, accuracy: 0.9051, f1: 0.8853, coppa: 0.8044\n",
      "Test result:\n",
      " test_loss: 0.6196, accuracy: 0.6935, f1: 0.3829, coppa: 0.2547\n",
      "Starting epoch number 57\n",
      "Loss for Training on Epoch 57 is 0.2515391707420349\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2397, accuracy: 0.9049, f1: 0.8878, coppa: 0.8053\n",
      "Test result:\n",
      " test_loss: 0.6533, accuracy: 0.6790, f1: 0.3317, coppa: 0.2124\n",
      "Starting epoch number 58\n",
      "Loss for Training on Epoch 58 is 0.25248828530311584\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2398, accuracy: 0.9060, f1: 0.8880, coppa: 0.8070\n",
      "Test result:\n",
      " test_loss: 0.6359, accuracy: 0.6932, f1: 0.3858, coppa: 0.2552\n",
      "Starting epoch number 59\n",
      "Loss for Training on Epoch 59 is 0.2588491439819336\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2385, accuracy: 0.9060, f1: 0.8876, coppa: 0.8069\n",
      "Test result:\n",
      " test_loss: 0.5691, accuracy: 0.7104, f1: 0.4438, coppa: 0.3051\n",
      "Starting epoch number 60\n",
      "Loss for Training on Epoch 60 is 0.2517475485801697\n",
      "Evaluating classifier\n",
      "Test result:\n",
      " test_loss: 0.2414, accuracy: 0.9025, f1: 0.8867, coppa: 0.8012\n",
      "Test result:\n",
      " test_loss: 0.6230, accuracy: 0.7008, f1: 0.4155, coppa: 0.2785\n",
      "2022-05-04 13:39:06 Index:1, Model: WMM, train_site: 3, test_site: 3, type: L8\n",
      "Starting epoch number 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 280.00 MiB (GPU 0; 24.00 GiB total capacity; 20.99 GiB already allocated; 0 bytes free; 21.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14264/1124230259.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mscheduler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfactor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'min'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_accs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_accs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_f1s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_f1s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTest_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriteria\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_frequency\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mmission\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmission_get_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14264/443558862.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(classifier, num_epochs, train_loader, val_loader, criterion, optimizer, scheduler, test_frequency)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Starting epoch number \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loss for Training on Epoch \"\u001b[0m \u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" is \"\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14264/443558862.py\u001b[0m in \u001b[0;36mtrain_net\u001b[1;34m(train_loader, classifier, criterion, optimizer)\u001b[0m\n\u001b[0;32m      6\u001b[0m       \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m       \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m       \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m       \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Wheat-Mapping\\Stage 2 Model Construction\\Models\\WMM.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mup_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeseq2img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2_connect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mup_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeseq2img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1_connect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mup_3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeseq2img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0_connect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Wheat-Mapping\\Stage 2 Model Construction\\Models\\WMM.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, down_sample_x)\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdown_sample_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdouble_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Wheat-Mapping\\Stage 2 Model Construction\\Models\\WMM.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[0midentity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mres_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0midentity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 447\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    441\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 443\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    444\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 280.00 MiB (GPU 0; 24.00 GiB total capacity; 20.99 GiB already allocated; 0 bytes free; 21.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for mission in train_mission_list:\n",
    "    mission.mission_start()\n",
    "    torch.cuda.empty_cache()\n",
    "    Train_data = Satellite_image_dataset(sites=mission.train_site, years=mission.train_years, type=mission.type, model=\"ConvLSTM\")\n",
    "    Train_dataloader = DataLoader(Train_data, batch_size, shuffle = True)\n",
    "    Test_data = Satellite_image_dataset(sites=mission.test_site, years = mission.test_years, type=mission.type, model=\"ConvLSTM\" )\n",
    "    Test_dataloader = DataLoader(Test_data, batch_size, shuffle = True)\n",
    "    if mission.type == 'L8':\n",
    "        net = WMM(n_channels = 60, n_classes =1, timesteps = 10, n_convlstm = 1, n_feature_maps=140).to(DEVICE)\n",
    "    else:\n",
    "        net = WMM(n_channels = 20, n_classes =1, timesteps = 10, n_convlstm = 1, n_feature_maps=140).to(DEVICE)\n",
    "    criteria = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), learning_rate,weight_decay= weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor = 0.25, patience = 3, mode='min', verbose=True)\n",
    "    classifier, train_losses, val_losses, train_accs, val_accs, train_f1s, val_f1s = train(net, num_epochs, Train_dataloader, Test_dataloader, criteria, optimizer, scheduler, test_frequency)\n",
    "\n",
    "    mission.mission_get_score([0,0,1,1],[1,0,0,1],[0,0,1,1],[1,0,0,1])\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
